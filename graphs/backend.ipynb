{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found edge starting at unknown node 'node_2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 105\u001b[0m\n\u001b[0;32m    101\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_edge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, END)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Compile the graph\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# app_graph = graph.compile(checkpointer=memory)\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m app_graph \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m display(Image(app_graph\u001b[38;5;241m.\u001b[39mget_graph()\u001b[38;5;241m.\u001b[39mdraw_mermaid_png()))\n",
      "File \u001b[1;32mc:\\Users\\toby_\\Imperial\\Year_3\\FYP\\Code\\venv\\Lib\\site-packages\\langgraph\\graph\\state.py:539\u001b[0m, in \u001b[0;36mStateGraph.compile\u001b[1;34m(self, checkpointer, store, interrupt_before, interrupt_after, debug, name)\u001b[0m\n\u001b[0;32m    536\u001b[0m interrupt_after \u001b[38;5;241m=\u001b[39m interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    538\u001b[0m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[1;32m--> 539\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[0;32m    548\u001b[0m output_channels \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__root__\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschemas[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    556\u001b[0m     ]\n\u001b[0;32m    557\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\toby_\\Imperial\\Year_3\\FYP\\Code\\venv\\Lib\\site-packages\\langgraph\\graph\\graph.py:374\u001b[0m, in \u001b[0;36mGraph.validate\u001b[1;34m(self, interrupt)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m all_sources:\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;129;01mand\u001b[39;00m source \u001b[38;5;241m!=\u001b[39m START:\n\u001b[1;32m--> 374\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound edge starting at unknown node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m START \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_sources:\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph must have an entrypoint: add at least one edge from START to another node\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    379\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found edge starting at unknown node 'node_2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# from langgraph.checkpoint import MemorySaver\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, RemoveMessage\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "# Initialize FastAPI\n",
    "# app = FastAPI()\n",
    "\n",
    "# Initialize models\n",
    "entrepreneur_llm = ChatMistralAI(\n",
    "    model=\"mistral-small-latest\", temperature=0, max_retries=2, streaming=True\n",
    ")\n",
    "\n",
    "dragon_llm = ChatMistralAI(\n",
    "    model=\"mistral-small-latest\", temperature=0, max_retries=2, streaming=True\n",
    ")\n",
    "\n",
    "# Define prompts\n",
    "entrepreneur_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an ambitious entrepreneur pitching a startup idea on Dragon's Den.\n",
    "    \n",
    "    Your task:\n",
    "    1. Identify a major problem in the {industry} industry.\n",
    "    2. Propose an innovative business idea that solves this problem.\n",
    "    3. Explain your unique selling point (USP) – what makes your idea different?\n",
    "    4. Describe your business model – how will you generate revenue?\n",
    "    \n",
    "    Be compelling and persuasive. Keep your pitch under 100 words.\n",
    "\n",
    "    Your pitch:\"\"\"\n",
    ")\n",
    "\n",
    "dragon_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are a seasoned investor on Dragon's Den, evaluating a startup pitch. \n",
    "    \n",
    "    Your task:\n",
    "    1. Analyze the business idea: {idea}\n",
    "    2. Identify the strengths – what makes it promising?\n",
    "    3. Highlight potential risks or weaknesses.\n",
    "    4. Assess market potential – is there demand? Who are the competitors?\n",
    "    5. Suggest improvements or alternative business strategies.\n",
    "\n",
    "    Provide your analysis in a professional but engaging way, like a real Dragon's Den judge. Keep your response under 200 words.\n",
    "\n",
    "    Your response:\"\"\"\n",
    ")\n",
    "\n",
    "# Define state for conversation tracking\n",
    "# class ConversationState(BaseModel):\n",
    "#     industry: str = \"\"\n",
    "#     pitch: str = \"\"\n",
    "#     evaluation: str = \"\"\n",
    "\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "# Initialize memory for storing conversation state\n",
    "# memory = MemorySaver()\n",
    "\n",
    "# Define LangGraph workflow\n",
    "# graph = StateGraph(ConversationState)\n",
    "graph = StateGraph(State)\n",
    "\n",
    "# Step 1: Generate entrepreneur's pitch\n",
    "def generate_pitch(state: State):\n",
    "    entrepreneur_chain = entrepreneur_prompt | entrepreneur_llm\n",
    "    response = entrepreneur_chain.invoke({\"industry\": state.industry})\n",
    "    state.pitch = response.content\n",
    "    return state\n",
    "\n",
    "# Step 2: Investor evaluates pitch\n",
    "def evaluate_pitch(state: State):\n",
    "    dragon_chain = dragon_prompt | dragon_llm\n",
    "    response = dragon_chain.invoke({\"idea\": state.pitch})\n",
    "    state.evaluation = response.content\n",
    "    return state\n",
    "\n",
    "@graph.node()\n",
    "async def entrepreneur_response(state: PitchState):\n",
    "    \"\"\"Entrepreneur responds to the investor's feedback.\"\"\"\n",
    "    response_chain = response_prompt | entrepreneur_llm\n",
    "    result = await response_chain.ainvoke({\"evaluation\": state.evaluation})\n",
    "    state.response = result.content\n",
    "    return state\n",
    "\n",
    "# Add nodes to the graph\n",
    "graph.add_node(\"entrepreneur\", RunnableLambda(generate_pitch))\n",
    "graph.add_node(\"investor\", RunnableLambda(evaluate_pitch))\n",
    "\n",
    "# Define edges (execution order)\n",
    "graph.add_edge(START, \"entrepreneur\")\n",
    "# graph.set_entry_point(\"entrepreneur\")\n",
    "graph.add_edge(\"entrepreneur\", \"investor\")  # Entrepreneur -> Investor\n",
    "graph.add_edge(\"investor\", END)\n",
    "\n",
    "# Compile the graph\n",
    "# app_graph = graph.compile(checkpointer=memory)\n",
    "app_graph = graph.compile()\n",
    "display(Image(app_graph.get_graph().draw_mermaid_png()))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
